#!/usr/bin/env python3
"""
å…³äºé¡µé¢å’Œéšç§é¡µé¢å†…å®¹æŠ“å–å™¨
"""

import requests
from pathlib import Path
from bs4 import BeautifulSoup
import json
import time

def scrape_pages():
    """æŠ“å–å…³äºé¡µé¢å’Œéšç§é¡µé¢çš„å®Œæ•´å†…å®¹"""
    
    pages = {
        'about': 'https://68tt.co/cn/about.html',
        'privacy': 'https://68tt.co/cn/privacy.html'
    }
    
    # æ¡Œé¢ç«¯å’Œç§»åŠ¨ç«¯User-Agent
    user_agents = {
        'desktop': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'mobile': 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("pages_analysis")
    output_dir.mkdir(exist_ok=True)
    
    results = {}
    
    for page_name, url in pages.items():
        print(f"\nğŸ” æŠ“å– {page_name} é¡µé¢: {url}")
        results[page_name] = {}
        
        for device, ua in user_agents.items():
            print(f"  ğŸ“± {device} ç‰ˆæœ¬...")
            
            try:
                headers = {'User-Agent': ua}
                response = requests.get(url, headers=headers, timeout=30)
                
                if response.status_code != 200:
                    print(f"    âŒ HTTP {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ä¿å­˜åŸå§‹HTML
                with open(output_dir / f"{page_name}_{device}_raw.html", 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                # åˆ†æé¡µé¢ç»“æ„
                page_data = {
                    'title': soup.title.string if soup.title else '',
                    'body_class': soup.body.get('class', []) if soup.body else [],
                    'main_content': [],
                    'images': [],
                    'text_sections': [],
                    'special_elements': []
                }
                
                # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
                main_wrapper = soup.find('div', class_='main-wrapper')
                if main_wrapper:
                    # æŸ¥æ‰¾å†…å®¹åŒºåŸŸ
                    content_areas = main_wrapper.find_all(['div', 'section', 'article'], 
                                                        class_=lambda x: x and any(
                                                            keyword in ' '.join(x).lower() 
                                                            for keyword in ['content', 'inner', 'main', 'about', 'privacy']
                                                        ))
                    
                    for area in content_areas:
                        area_info = {
                            'tag': area.name,
                            'classes': area.get('class', []),
                            'id': area.get('id', ''),
                            'text_content': area.get_text(strip=True)[:500],
                            'html_snippet': str(area)[:1000],
                            'child_count': len(area.find_all()),
                            'images': []
                        }
                        
                        # æŸ¥æ‰¾å›¾ç‰‡
                        images = area.find_all('img')
                        for img in images:
                            img_info = {
                                'src': img.get('src', ''),
                                'alt': img.get('alt', ''),
                                'class': img.get('class', [])
                            }
                            area_info['images'].append(img_info)
                            page_data['images'].append(img_info)
                        
                        page_data['main_content'].append(area_info)
                
                # æŸ¥æ‰¾æ‰€æœ‰æ–‡æœ¬æ®µè½
                text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div'], 
                                            string=lambda text: text and len(text.strip()) > 20)
                
                for elem in text_elements[:10]:  # é™åˆ¶å‰10ä¸ª
                    text_info = {
                        'tag': elem.name,
                        'class': elem.get('class', []),
                        'text': elem.get_text(strip=True)[:200],
                        'parent_class': elem.parent.get('class', []) if elem.parent else []
                    }
                    page_data['text_sections'].append(text_info)
                
                # æŸ¥æ‰¾ç‰¹æ®Šå…ƒç´ 
                special_selectors = [
                    'div[class*="about"]',
                    'div[class*="privacy"]', 
                    'div[class*="policy"]',
                    'div[class*="content"]',
                    'div[class*="text"]',
                    'div[class*="info"]'
                ]
                
                for selector in special_selectors:
                    elements = soup.select(selector)
                    for elem in elements:
                        special_info = {
                            'selector': selector,
                            'tag': elem.name,
                            'classes': elem.get('class', []),
                            'id': elem.get('id', ''),
                            'text_preview': elem.get_text(strip=True)[:100]
                        }
                        page_data['special_elements'].append(special_info)
                
                results[page_name][device] = page_data
                
                print(f"    âœ… æˆåŠŸ: {len(page_data['main_content'])} ä¸ªå†…å®¹åŒºåŸŸ")
                print(f"       å›¾ç‰‡: {len(page_data['images'])} ä¸ª")
                print(f"       æ–‡æœ¬æ®µè½: {len(page_data['text_sections'])} ä¸ª")
                
            except Exception as e:
                print(f"    âŒ å¤±è´¥: {e}")
                results[page_name][device] = {'error': str(e)}
    
    # ä¿å­˜åˆ†æç»“æœ
    with open(output_dir / "pages_analysis.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_pages_report(results, output_dir)
    
    print(f"\nğŸ“Š é¡µé¢åˆ†æå®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“‹ åˆ†æç»“æœ: pages_analysis.json")
    print(f"ğŸ“ é¡µé¢æŠ¥å‘Š: pages_report.md")

def generate_pages_report(results, output_dir):
    """ç”Ÿæˆé¡µé¢åˆ†ææŠ¥å‘Š"""
    
    report = """# å…³äºé¡µé¢å’Œéšç§é¡µé¢åˆ†ææŠ¥å‘Š

## ğŸ“‹ é¡µé¢æ¦‚è§ˆ

"""
    
    for page_name, page_data in results.items():
        report += f"""## ğŸ“„ {page_name.title()} é¡µé¢

"""
        
        for device, data in page_data.items():
            if 'error' in data:
                report += f"""### {device.title()} ç‰ˆæœ¬
âŒ æŠ“å–å¤±è´¥: {data['error']}

"""
                continue
            
            report += f"""### {device.title()} ç‰ˆæœ¬
- **é¡µé¢æ ‡é¢˜**: {data['title']}
- **Bodyç±»**: {', '.join(data['body_class'])}
- **ä¸»è¦å†…å®¹åŒºåŸŸ**: {len(data['main_content'])} ä¸ª
- **å›¾ç‰‡æ•°é‡**: {len(data['images'])} ä¸ª
- **æ–‡æœ¬æ®µè½**: {len(data['text_sections'])} ä¸ª

#### ä¸»è¦å†…å®¹åŒºåŸŸ:
"""
            
            for i, area in enumerate(data['main_content'], 1):
                report += f"""
**åŒºåŸŸ {i}**: `{area['tag']}.{'.'.join(area['classes'])}`
- ID: `{area['id']}`
- å­å…ƒç´ : {area['child_count']} ä¸ª
- å›¾ç‰‡: {len(area['images'])} ä¸ª
- å†…å®¹é¢„è§ˆ: {area['text_content'][:100]}...

"""
                
                if area['images']:
                    report += "**å›¾ç‰‡èµ„æº**:\n"
                    for img in area['images']:
                        report += f"- `{img['src']}` - Alt: `{img['alt']}`\n"
                    report += "\n"
            
            if data['text_sections']:
                report += "#### ä¸»è¦æ–‡æœ¬å†…å®¹:\n"
                for text in data['text_sections'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    report += f"- **{text['tag']}.{'.'.join(text['class'])}**: {text['text'][:80]}...\n"
                report += "\n"
            
            if data['special_elements']:
                report += "#### ç‰¹æ®Šå…ƒç´ :\n"
                unique_elements = {}
                for elem in data['special_elements']:
                    key = f"{elem['tag']}.{'.'.join(elem['classes'])}"
                    if key not in unique_elements:
                        unique_elements[key] = elem
                
                for elem in list(unique_elements.values())[:5]:
                    report += f"- `{elem['tag']}.{'.'.join(elem['classes'])}` - {elem['text_preview'][:50]}...\n"
                report += "\n"
    
    report += """## ğŸ”§ ä¿®å¤å»ºè®®

### 1. é¡µé¢ç»“æ„å¯¹æ¯”
- æ¯”è¾ƒå½“å‰é¡µé¢ä¸æºç«™ç‚¹çš„HTMLç»“æ„
- è¯†åˆ«ç¼ºå¤±æˆ–å¤šä½™çš„å†…å®¹åŒºåŸŸ
- ç¡®ä¿CSSç±»åå’ŒIDä¸€è‡´

### 2. å†…å®¹åŒæ­¥
- æ›´æ–°é¡µé¢æ ‡é¢˜å’Œmetaä¿¡æ¯
- åŒæ­¥æ‰€æœ‰æ–‡æœ¬å†…å®¹
- ä¸‹è½½å¹¶æ›¿æ¢å›¾ç‰‡èµ„æº

### 3. æ ·å¼ä¸€è‡´æ€§
- ç¡®ä¿CSSæ ·å¼ä¸æºç«™ç‚¹åŒ¹é…
- æ£€æŸ¥å“åº”å¼è®¾è®¡åœ¨ç§»åŠ¨ç«¯çš„è¡¨ç°
- éªŒè¯å­—ä½“ã€é¢œè‰²ã€é—´è·ç­‰è§†è§‰å…ƒç´ 

### 4. åŠŸèƒ½å®Œæ•´æ€§
- ç¡®ä¿æ‰€æœ‰é“¾æ¥å’Œäº¤äº’åŠŸèƒ½æ­£å¸¸
- éªŒè¯è¡¨å•å’ŒæŒ‰é’®çš„è¡Œä¸º
- æµ‹è¯•åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„å…¼å®¹æ€§
"""
    
    with open(output_dir / "pages_report.md", 'w', encoding='utf-8') as f:
        f.write(report)

if __name__ == "__main__":
    scrape_pages()
