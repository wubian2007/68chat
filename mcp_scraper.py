#!/usr/bin/env python3
"""
MCP (Model Context Protocol) é£æ ¼çš„ç½‘ç«™æŠ“å–å·¥å…·
ä¸“é—¨ä¸º 68tt.co ç½‘ç«™è®¾è®¡çš„è½»é‡çº§æŠ“å–å™¨
"""

import asyncio
import aiohttp
import json
import os
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import time

class MCPScraper:
    def __init__(self):
        self.base_url = "https://68tt.co/cn/"
        self.output_dir = Path("mcp_scraped")
        self.session = None
        self.scraped_content = {}
        
    async def initialize(self):
        """åˆå§‹åŒ–å¼‚æ­¥ä¼šè¯"""
        connector = aiohttp.TCPConnector(limit=10)
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'MCP-Scraper/1.0 (68tt.co content extraction)'
            }
        )
        self.output_dir.mkdir(exist_ok=True)
        
    async def close(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url):
        """å¼‚æ­¥è·å–é¡µé¢å†…å®¹"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    return {
                        'url': url,
                        'status': response.status,
                        'content': content,
                        'headers': dict(response.headers)
                    }
                else:
                    return {
                        'url': url,
                        'status': response.status,
                        'error': f'HTTP {response.status}'
                    }
        except Exception as e:
            return {
                'url': url,
                'status': 0,
                'error': str(e)
            }
    
    async def fetch_asset(self, url):
        """å¼‚æ­¥è·å–é™æ€èµ„æº"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    return {
                        'url': url,
                        'status': response.status,
                        'content': content,
                        'content_type': response.headers.get('content-type', '')
                    }
                else:
                    return {'url': url, 'status': response.status, 'error': f'HTTP {response.status}'}
        except Exception as e:
            return {'url': url, 'status': 0, 'error': str(e)}
    
    def extract_content(self, html_content):
        """æå–é¡µé¢ç»“æ„åŒ–å†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–ä¸»è¦å†…å®¹
        content_data = {
            'title': soup.title.string if soup.title else '',
            'meta_description': '',
            'headings': [],
            'paragraphs': [],
            'links': [],
            'images': [],
            'scripts': [],
            'stylesheets': []
        }
        
        # Metaæè¿°
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            content_data['meta_description'] = meta_desc.get('content', '')
        
        # æ ‡é¢˜
        for i in range(1, 7):
            headings = soup.find_all(f'h{i}')
            for h in headings:
                content_data['headings'].append({
                    'level': i,
                    'text': h.get_text().strip(),
                    'id': h.get('id', '')
                })
        
        # æ®µè½
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text().strip()
            if text:
                content_data['paragraphs'].append(text)
        
        # é“¾æ¥
        links = soup.find_all('a', href=True)
        for link in links:
            content_data['links'].append({
                'href': link['href'],
                'text': link.get_text().strip(),
                'title': link.get('title', '')
            })
        
        # å›¾ç‰‡
        images = soup.find_all('img', src=True)
        for img in images:
            content_data['images'].append({
                'src': img['src'],
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
        
        # æ ·å¼è¡¨
        stylesheets = soup.find_all('link', rel='stylesheet')
        for css in stylesheets:
            if css.get('href'):
                content_data['stylesheets'].append(css['href'])
        
        # JavaScript
        scripts = soup.find_all('script', src=True)
        for script in scripts:
            content_data['scripts'].append(script['src'])
        
        return content_data
    
    async def scrape_main_pages(self):
        """æŠ“å–ä¸»è¦é¡µé¢"""
        pages_to_scrape = [
            self.base_url,
            urljoin(self.base_url, '/about'),
            urljoin(self.base_url, '/privacy'),
            urljoin(self.base_url, '/contact'),
        ]
        
        print("ğŸš€ å¼€å§‹æŠ“å–ä¸»è¦é¡µé¢...")
        
        tasks = [self.fetch_page(url) for url in pages_to_scrape]
        results = await asyncio.gather(*tasks)
        
        scraped_pages = {}
        
        for result in results:
            if 'content' in result:
                print(f"âœ… æˆåŠŸæŠ“å–: {result['url']}")
                
                # æå–ç»“æ„åŒ–å†…å®¹
                extracted = self.extract_content(result['content'])
                
                scraped_pages[result['url']] = {
                    'raw_html': result['content'],
                    'extracted_content': extracted,
                    'status': result['status'],
                    'headers': result.get('headers', {})
                }
                
                # ä¿å­˜åŸå§‹HTML
                filename = self.url_to_filename(result['url'])
                html_path = self.output_dir / f"{filename}.html"
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(result['content'])
                
                # ä¿å­˜ç»“æ„åŒ–æ•°æ®
                json_path = self.output_dir / f"{filename}.json"
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(extracted, f, ensure_ascii=False, indent=2)
                    
            else:
                print(f"âŒ æŠ“å–å¤±è´¥: {result['url']} - {result.get('error', 'Unknown error')}")
        
        return scraped_pages
    
    async def download_assets(self, pages_data):
        """ä¸‹è½½é™æ€èµ„æº"""
        assets_to_download = set()
        
        # æ”¶é›†æ‰€æœ‰èµ„æºURL
        for url, page_data in pages_data.items():
            extracted = page_data['extracted_content']
            
            # å›¾ç‰‡
            for img in extracted['images']:
                img_url = urljoin(url, img['src'])
                assets_to_download.add(img_url)
            
            # æ ·å¼è¡¨
            for css in extracted['stylesheets']:
                css_url = urljoin(url, css)
                assets_to_download.add(css_url)
            
            # JavaScript
            for js in extracted['scripts']:
                js_url = urljoin(url, js)
                assets_to_download.add(js_url)
        
        print(f"ğŸ“¦ å‘ç° {len(assets_to_download)} ä¸ªé™æ€èµ„æº")
        
        # åˆ›å»ºèµ„æºç›®å½•
        assets_dir = self.output_dir / "assets"
        assets_dir.mkdir(exist_ok=True)
        
        # ä¸‹è½½èµ„æº
        downloaded_assets = {}
        
        for asset_url in assets_to_download:
            if self.is_same_domain(asset_url):
                result = await self.fetch_asset(asset_url)
                
                if 'content' in result:
                    filename = self.url_to_filename(asset_url, keep_extension=True)
                    asset_path = assets_dir / filename
                    
                    with open(asset_path, 'wb') as f:
                        f.write(result['content'])
                    
                    downloaded_assets[asset_url] = {
                        'local_path': str(asset_path),
                        'content_type': result['content_type'],
                        'size': len(result['content'])
                    }
                    
                    print(f"ğŸ“¥ ä¸‹è½½èµ„æº: {filename}")
                else:
                    print(f"âŒ èµ„æºä¸‹è½½å¤±è´¥: {asset_url}")
        
        return downloaded_assets
    
    def is_same_domain(self, url):
        """æ£€æŸ¥æ˜¯å¦åŒåŸŸå"""
        return urlparse(url).netloc == urlparse(self.base_url).netloc
    
    def url_to_filename(self, url, keep_extension=False):
        """URLè½¬æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        if not path:
            return 'index'
        
        # æ¸…ç†æ–‡ä»¶å
        filename = path.replace('/', '_').replace('?', '_').replace('&', '_')
        
        if keep_extension:
            return filename
        else:
            return os.path.splitext(filename)[0] or 'index'
    
    async def generate_mcp_report(self, pages_data, assets_data):
        """ç”ŸæˆMCPæ ¼å¼çš„æŠ¥å‘Š"""
        report = {
            'scraping_info': {
                'target_url': self.base_url,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_pages': len(pages_data),
                'total_assets': len(assets_data),
                'output_directory': str(self.output_dir)
            },
            'pages': {},
            'assets': assets_data,
            'summary': {
                'successful_pages': len([p for p in pages_data.values() if p['status'] == 200]),
                'failed_pages': len([p for p in pages_data.values() if p['status'] != 200]),
                'total_content_size': sum(len(p['raw_html']) for p in pages_data.values()),
                'unique_images': len(set(img['src'] for page in pages_data.values() 
                                      for img in page['extracted_content']['images'])),
                'unique_links': len(set(link['href'] for page in pages_data.values() 
                                     for link in page['extracted_content']['links']))
            }
        }
        
        # ç®€åŒ–é¡µé¢æ•°æ®ç”¨äºæŠ¥å‘Š
        for url, page_data in pages_data.items():
            report['pages'][url] = {
                'title': page_data['extracted_content']['title'],
                'status': page_data['status'],
                'content_length': len(page_data['raw_html']),
                'headings_count': len(page_data['extracted_content']['headings']),
                'paragraphs_count': len(page_data['extracted_content']['paragraphs']),
                'images_count': len(page_data['extracted_content']['images']),
                'links_count': len(page_data['extracted_content']['links'])
            }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / 'mcp_scraping_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report
    
    async def run(self):
        """è¿è¡ŒMCPæŠ“å–å™¨"""
        await self.initialize()
        
        try:
            print("ğŸ”§ MCP 68tt.co ç½‘ç«™å†…å®¹æŠ“å–å™¨")
            print("=" * 50)
            
            # æŠ“å–é¡µé¢
            pages_data = await self.scrape_main_pages()
            
            # ä¸‹è½½èµ„æº
            assets_data = await self.download_assets(pages_data)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = await self.generate_mcp_report(pages_data, assets_data)
            
            print("\nâœ… MCP æŠ“å–å®Œæˆ!")
            print(f"ğŸ“Š é¡µé¢: {report['summary']['successful_pages']} æˆåŠŸ, {report['summary']['failed_pages']} å¤±è´¥")
            print(f"ğŸ“¦ èµ„æº: {len(assets_data)} ä¸ªæ–‡ä»¶")
            print(f"ğŸ’¾ æ€»å¤§å°: {report['summary']['total_content_size']:,} å­—èŠ‚")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            
        finally:
            await self.close()

async def main():
    """ä¸»å‡½æ•°"""
    scraper = MCPScraper()
    await scraper.run()

if __name__ == "__main__":
    asyncio.run(main())
