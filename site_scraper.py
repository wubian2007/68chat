#!/usr/bin/env python3
"""
68tt.co ç½‘ç«™å†…å®¹æŠ“å–å·¥å…·
ä½¿ç”¨ requests, BeautifulSoup å’Œ urllib å®ç°å®Œæ•´ç½‘ç«™å…‹éš†
"""

import os
import sys
import requests
from urllib.parse import urljoin, urlparse, unquote
from bs4 import BeautifulSoup
import time
import json
from pathlib import Path
import mimetypes
import re

class WebsiteScraper:
    def __init__(self, base_url, output_dir="scraped_site"):
        self.base_url = base_url.rstrip('/')
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        self.downloaded_urls = set()
        self.failed_urls = set()
        self.site_map = {}
        
    def clean_filename(self, url):
        """æ¸…ç†URLç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = unquote(parsed.path)
        
        if path == '/' or path == '':
            return 'index.html'
        
        # ç§»é™¤å¼€å¤´çš„æ–œæ 
        path = path.lstrip('/')
        
        # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œæ·»åŠ .html
        if not os.path.splitext(path)[1]:
            if not path.endswith('/'):
                path += '.html'
            else:
                path += 'index.html'
        
        # æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
        path = re.sub(r'[<>:"|?*]', '_', path)
        return path
    
    def download_file(self, url, local_path):
        """ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°è·¯å¾„"""
        try:
            print(f"ä¸‹è½½: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            with open(local_path, 'wb') as f:
                f.write(response.content)
            
            self.downloaded_urls.add(url)
            return True
            
        except Exception as e:
            print(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            return False
    
    def process_html(self, html_content, base_url):
        """å¤„ç†HTMLå†…å®¹ï¼Œä¸‹è½½èµ„æºå¹¶æ›´æ–°é“¾æ¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # å¤„ç†å›¾ç‰‡
        for img in soup.find_all('img'):
            src = img.get('src')
            if src:
                img_url = urljoin(base_url, src)
                if self.is_same_domain(img_url):
                    local_path = self.url_to_local_path(img_url)
                    if self.download_file(img_url, local_path):
                        img['src'] = self.local_path_to_relative(local_path)
        
        # å¤„ç†CSSé“¾æ¥
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href:
                css_url = urljoin(base_url, href)
                if self.is_same_domain(css_url):
                    local_path = self.url_to_local_path(css_url)
                    if self.download_file(css_url, local_path):
                        link['href'] = self.local_path_to_relative(local_path)
        
        # å¤„ç†JavaScript
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src:
                js_url = urljoin(base_url, src)
                if self.is_same_domain(js_url):
                    local_path = self.url_to_local_path(js_url)
                    if self.download_file(js_url, local_path):
                        script['src'] = self.local_path_to_relative(local_path)
        
        # å¤„ç†é¡µé¢é“¾æ¥
        for a in soup.find_all('a', href=True):
            href = a.get('href')
            if href and not href.startswith('#') and not href.startswith('mailto:'):
                page_url = urljoin(base_url, href)
                if self.is_same_domain(page_url):
                    local_path = self.url_to_local_path(page_url)
                    a['href'] = self.local_path_to_relative(local_path)
        
        return str(soup)
    
    def is_same_domain(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å±äºåŒä¸€åŸŸå"""
        return urlparse(url).netloc == urlparse(self.base_url).netloc
    
    def url_to_local_path(self, url):
        """å°†URLè½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„"""
        filename = self.clean_filename(url)
        return self.output_dir / filename
    
    def local_path_to_relative(self, local_path):
        """å°†æœ¬åœ°è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„"""
        return os.path.relpath(local_path, self.output_dir)
    
    def scrape_page(self, url):
        """æŠ“å–å•ä¸ªé¡µé¢"""
        if url in self.downloaded_urls:
            return
        
        try:
            print(f"æŠ“å–é¡µé¢: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # å¤„ç†HTMLå†…å®¹
            processed_html = self.process_html(response.text, url)
            
            # ä¿å­˜é¡µé¢
            local_path = self.url_to_local_path(url)
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(local_path, 'w', encoding='utf-8') as f:
                f.write(processed_html)
            
            self.downloaded_urls.add(url)
            self.site_map[url] = str(local_path)
            
            print(f"âœ… æˆåŠŸä¿å­˜: {local_path}")
            
        except Exception as e:
            print(f"âŒ é¡µé¢æŠ“å–å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
    
    def discover_pages(self, start_url):
        """å‘ç°ç½‘ç«™ä¸­çš„æ‰€æœ‰é¡µé¢"""
        pages_to_scrape = []
        
        try:
            response = self.session.get(start_url, timeout=30)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰å†…éƒ¨é“¾æ¥
            for a in soup.find_all('a', href=True):
                href = a.get('href')
                if href and not href.startswith('#') and not href.startswith('mailto:'):
                    page_url = urljoin(start_url, href)
                    if self.is_same_domain(page_url) and page_url not in pages_to_scrape:
                        pages_to_scrape.append(page_url)
            
        except Exception as e:
            print(f"é¡µé¢å‘ç°å¤±è´¥: {e}")
        
        return pages_to_scrape
    
    def scrape_website(self):
        """æŠ“å–æ•´ä¸ªç½‘ç«™"""
        print(f"ğŸš€ å¼€å§‹æŠ“å–ç½‘ç«™: {self.base_url}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # å‘ç°æ‰€æœ‰é¡µé¢
        pages = [self.base_url]
        discovered_pages = self.discover_pages(self.base_url)
        pages.extend(discovered_pages)
        
        # æ·»åŠ å¸¸è§é¡µé¢
        common_pages = ['/about', '/privacy', '/contact', '/help']
        for page in common_pages:
            full_url = self.base_url + page
            if full_url not in pages:
                pages.append(full_url)
        
        print(f"ğŸ“„ å‘ç° {len(pages)} ä¸ªé¡µé¢")
        
        # æŠ“å–æ‰€æœ‰é¡µé¢
        for i, page_url in enumerate(pages, 1):
            print(f"\n[{i}/{len(pages)}] å¤„ç†é¡µé¢...")
            self.scrape_page(page_url)
            time.sleep(1)  # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        print(f"\nâœ… æŠ“å–å®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸ: {len(self.downloaded_urls)} ä¸ªæ–‡ä»¶")
        print(f"âŒ å¤±è´¥: {len(self.failed_urls)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {self.output_dir}")
    
    def generate_report(self):
        """ç”ŸæˆæŠ“å–æŠ¥å‘Š"""
        report = {
            'base_url': self.base_url,
            'total_downloaded': len(self.downloaded_urls),
            'total_failed': len(self.failed_urls),
            'downloaded_urls': list(self.downloaded_urls),
            'failed_urls': list(self.failed_urls),
            'site_map': self.site_map,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        report_path = self.output_dir / 'scrape_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆç®€å•çš„HTMLæŠ¥å‘Š
        html_report = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>ç½‘ç«™æŠ“å–æŠ¥å‘Š</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .success {{ color: green; }}
                .error {{ color: red; }}
                .info {{ color: blue; }}
                ul {{ list-style-type: none; }}
                li {{ margin: 5px 0; }}
            </style>
        </head>
        <body>
            <h1>ç½‘ç«™æŠ“å–æŠ¥å‘Š</h1>
            <p class="info">ç›®æ ‡ç½‘ç«™: {self.base_url}</p>
            <p class="info">æŠ“å–æ—¶é—´: {report['timestamp']}</p>
            
            <h2 class="success">æˆåŠŸä¸‹è½½ ({len(self.downloaded_urls)} ä¸ªæ–‡ä»¶)</h2>
            <ul>
                {''.join([f'<li>âœ… {url}</li>' for url in sorted(self.downloaded_urls)])}
            </ul>
            
            <h2 class="error">ä¸‹è½½å¤±è´¥ ({len(self.failed_urls)} ä¸ªæ–‡ä»¶)</h2>
            <ul>
                {''.join([f'<li>âŒ {url}</li>' for url in sorted(self.failed_urls)])}
            </ul>
        </body>
        </html>
        """
        
        html_report_path = self.output_dir / 'scrape_report.html'
        with open(html_report_path, 'w', encoding='utf-8') as f:
            f.write(html_report)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1:
        target_url = sys.argv[1]
    else:
        target_url = "https://68tt.co/cn/"
    
    if len(sys.argv) > 2:
        output_dir = sys.argv[2]
    else:
        output_dir = "scraped_68tt"
    
    print("ğŸ”§ 68tt.co ç½‘ç«™å†…å®¹æŠ“å–å·¥å…·")
    print("=" * 50)
    
    scraper = WebsiteScraper(target_url, output_dir)
    scraper.scrape_website()

if __name__ == "__main__":
    main()
