#!/usr/bin/env python3
"""
68tt.co ç»¼åˆç½‘ç«™æŠ“å–å·¥å…·
æ•´åˆ Firecrawl MCPã€ç®€å•æŠ“å–å’Œèµ„æºä¸‹è½½åŠŸèƒ½
"""

import asyncio
import requests
import json
import os
import time
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import base64

class ComprehensiveScraper:
    def __init__(self, output_dir="comprehensive_output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.assets_dir = self.output_dir / "assets"
        self.screenshots_dir = self.output_dir / "screenshots"
        self.html_dir = self.output_dir / "html"
        self.markdown_dir = self.output_dir / "markdown"
        
        for dir_path in [self.assets_dir, self.screenshots_dir, self.html_dir, self.markdown_dir]:
            dir_path.mkdir(exist_ok=True)
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        self.downloaded_images = set()
        self.scraped_pages = []
        
    def scrape_page(self, url):
        """æŠ“å–å•ä¸ªé¡µé¢"""
        print(f"ğŸ” æŠ“å–é¡µé¢: {url}")
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            if response.status_code != 200:
                print(f"âŒ é¡µé¢è·å–å¤±è´¥: HTTP {response.status_code}")
                return None
            
            print(f"âœ… é¡µé¢è·å–æˆåŠŸ: {len(response.text)} å­—ç¬¦")
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.title.string if soup.title else "Untitled"
            
            # ç”Ÿæˆæ–‡ä»¶å
            parsed_url = urlparse(url)
            filename_base = parsed_url.path.strip('/').replace('/', '_') or 'index'
            if parsed_url.query:
                filename_base += '_' + parsed_url.query.replace('&', '_').replace('=', '_')
            
            page_data = {
                'url': url,
                'title': title.strip(),
                'filename_base': filename_base,
                'html_content': response.text,
                'soup': soup,
                'content_length': len(response.text)
            }
            
            return page_data
            
        except Exception as e:
            print(f"âŒ é¡µé¢æŠ“å–é”™è¯¯: {e}")
            return None
    
    def save_html(self, page_data):
        """ä¿å­˜HTMLæ–‡ä»¶"""
        filename = f"{page_data['filename_base']}.html"
        html_path = self.html_dir / filename
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(page_data['html_content'])
        
        print(f"ğŸ“„ HTMLä¿å­˜: {filename}")
        return html_path
    
    def extract_and_save_markdown(self, page_data):
        """æå–å¹¶ä¿å­˜Markdownæ ¼å¼å†…å®¹"""
        soup = page_data['soup']
        
        # æå–ä¸»è¦å†…å®¹
        markdown_content = []
        
        # æ ‡é¢˜
        if soup.title:
            markdown_content.append(f"# {soup.title.string}\n")
        
        # ä¸»è¦æ–‡æœ¬å†…å®¹
        for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div']):
            text = element.get_text().strip()
            if text and len(text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                if element.name.startswith('h'):
                    level = int(element.name[1])
                    markdown_content.append(f"{'#' * level} {text}\n")
                else:
                    markdown_content.append(f"{text}\n")
        
        # é“¾æ¥
        links = soup.find_all('a', href=True)
        if links:
            markdown_content.append("\n## é“¾æ¥\n")
            for link in links:
                href = link['href']
                text = link.get_text().strip()
                if text and href:
                    markdown_content.append(f"- [{text}]({href})")
        
        # å›¾ç‰‡
        images = soup.find_all('img', src=True)
        if images:
            markdown_content.append("\n## å›¾ç‰‡\n")
            for img in images:
                src = img['src']
                alt = img.get('alt', '')
                markdown_content.append(f"![{alt}]({src})")
        
        # ä¿å­˜Markdown
        markdown_text = '\n'.join(markdown_content)
        filename = f"{page_data['filename_base']}.md"
        md_path = self.markdown_dir / filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(markdown_text)
        
        print(f"ğŸ“ Markdownä¿å­˜: {filename}")
        return md_path
    
    def download_images(self, page_data):
        """ä¸‹è½½é¡µé¢ä¸­çš„å›¾ç‰‡"""
        soup = page_data['soup']
        images = soup.find_all('img', src=True)
        
        print(f"ğŸ–¼ï¸  å‘ç° {len(images)} ä¸ªå›¾ç‰‡")
        downloaded_count = 0
        
        for i, img in enumerate(images):
            img_src = img['src']
            
            # å¤„ç†ç›¸å¯¹URL
            if img_src.startswith('//'):
                img_url = 'https:' + img_src
            elif img_src.startswith('/'):
                img_url = 'https://68tt.co' + img_src
            elif not img_src.startswith('http'):
                img_url = urljoin(page_data['url'], img_src)
            else:
                img_url = img_src
            
            # è·³è¿‡base64å’Œå·²ä¸‹è½½çš„å›¾ç‰‡
            if img_url.startswith('data:') or img_url in self.downloaded_images:
                continue
            
            try:
                print(f"  ğŸ“¥ ä¸‹è½½å›¾ç‰‡ {i+1}: {os.path.basename(urlparse(img_url).path)}")
                img_response = requests.get(img_url, headers=self.headers, timeout=30)
                
                if img_response.status_code == 200:
                    # ç”Ÿæˆæ–‡ä»¶å
                    img_filename = os.path.basename(urlparse(img_url).path)
                    if not img_filename or '.' not in img_filename:
                        content_type = img_response.headers.get('content-type', '')
                        if 'png' in content_type:
                            ext = '.png'
                        elif 'jpeg' in content_type or 'jpg' in content_type:
                            ext = '.jpg'
                        elif 'gif' in content_type:
                            ext = '.gif'
                        elif 'svg' in content_type:
                            ext = '.svg'
                        else:
                            ext = '.jpg'
                        img_filename = f"image_{hash(img_url) % 10000}{ext}"
                    
                    # ç¡®ä¿æ–‡ä»¶åå”¯ä¸€
                    counter = 1
                    original_filename = img_filename
                    while (self.assets_dir / img_filename).exists():
                        name, ext = os.path.splitext(original_filename)
                        img_filename = f"{name}_{counter}{ext}"
                        counter += 1
                    
                    img_path = self.assets_dir / img_filename
                    with open(img_path, 'wb') as f:
                        f.write(img_response.content)
                    
                    self.downloaded_images.add(img_url)
                    downloaded_count += 1
                    print(f"    âœ… ä¿å­˜: {img_filename} ({len(img_response.content)} å­—èŠ‚)")
                else:
                    print(f"    âŒ ä¸‹è½½å¤±è´¥: HTTP {img_response.status_code}")
                    
            except Exception as e:
                print(f"    âŒ å›¾ç‰‡ä¸‹è½½é”™è¯¯: {e}")
            
            # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            time.sleep(0.2)
        
        print(f"ğŸ“¥ æˆåŠŸä¸‹è½½ {downloaded_count} ä¸ªå›¾ç‰‡")
        return downloaded_count
    
    def take_screenshot_simulation(self, page_data):
        """æ¨¡æ‹Ÿæˆªå›¾åŠŸèƒ½ï¼ˆåˆ›å»ºé¡µé¢é¢„è§ˆï¼‰"""
        # ç”±äºæˆ‘ä»¬æ— æ³•çœŸæ­£æˆªå›¾ï¼Œåˆ›å»ºä¸€ä¸ªHTMLé¢„è§ˆæ–‡ä»¶
        preview_html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>é¡µé¢é¢„è§ˆ: {page_data['title']}</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
                .preview {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                .header {{ border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }}
                .url {{ color: #666; font-size: 14px; }}
                .title {{ color: #333; font-size: 18px; font-weight: bold; margin: 10px 0; }}
                .content {{ line-height: 1.6; }}
                .stats {{ background: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 20px; }}
            </style>
        </head>
        <body>
            <div class="preview">
                <div class="header">
                    <div class="url">{page_data['url']}</div>
                    <div class="title">{page_data['title']}</div>
                </div>
                <div class="content">
                    <p>é¡µé¢å†…å®¹é•¿åº¦: {page_data['content_length']} å­—ç¬¦</p>
                    <p>æŠ“å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                <div class="stats">
                    <strong>é¡µé¢ç»Ÿè®¡:</strong><br>
                    - å›¾ç‰‡æ•°é‡: {len(page_data['soup'].find_all('img'))} ä¸ª<br>
                    - é“¾æ¥æ•°é‡: {len(page_data['soup'].find_all('a', href=True))} ä¸ª<br>
                    - æ®µè½æ•°é‡: {len(page_data['soup'].find_all('p'))} ä¸ª
                </div>
            </div>
        </body>
        </html>
        """
        
        filename = f"{page_data['filename_base']}_preview.html"
        preview_path = self.screenshots_dir / filename
        
        with open(preview_path, 'w', encoding='utf-8') as f:
            f.write(preview_html)
        
        print(f"ğŸ“¸ é¡µé¢é¢„è§ˆä¿å­˜: {filename}")
        return preview_path
    
    def scrape_website(self):
        """æŠ“å–æ•´ä¸ªç½‘ç«™"""
        print("ğŸš€ å¼€å§‹ç»¼åˆç½‘ç«™æŠ“å–...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # è¦æŠ“å–çš„é¡µé¢
        pages = [
            "https://68tt.co/cn/",
            "https://68tt.co/cn/about.html",
            "https://68tt.co/cn/privacy.html",
            "https://68tt.co/cn/enterprise.html"
        ]
        
        total_images = 0
        
        for i, url in enumerate(pages, 1):
            print(f"\n{'='*60}")
            print(f"[{i}/{len(pages)}] å¤„ç†é¡µé¢")
            
            # æŠ“å–é¡µé¢
            page_data = self.scrape_page(url)
            if not page_data:
                continue
            
            # ä¿å­˜HTML
            self.save_html(page_data)
            
            # ä¿å­˜Markdown
            self.extract_and_save_markdown(page_data)
            
            # ä¸‹è½½å›¾ç‰‡
            img_count = self.download_images(page_data)
            total_images += img_count
            
            # åˆ›å»ºé¡µé¢é¢„è§ˆ
            self.take_screenshot_simulation(page_data)
            
            # è®°å½•é¡µé¢ä¿¡æ¯
            self.scraped_pages.append({
                'url': page_data['url'],
                'title': page_data['title'],
                'content_length': page_data['content_length'],
                'images_downloaded': img_count
            })
            
            # é¡µé¢é—´åœé¡¿
            if i < len(pages):
                time.sleep(2)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report(total_images)
    
    def generate_final_report(self, total_images):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print("ğŸ“Š ç»¼åˆæŠ“å–å®Œæˆç»Ÿè®¡")
        
        # æ–‡ä»¶ç»Ÿè®¡
        all_files = list(self.output_dir.rglob("*"))
        files = [f for f in all_files if f.is_file()]
        
        html_files = [f for f in files if f.suffix == '.html']
        md_files = [f for f in files if f.suffix == '.md']
        img_files = [f for f in files if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']]
        
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(f.stat().st_size for f in files)
        
        print(f"ğŸ“„ HTML æ–‡ä»¶: {len(html_files)} ä¸ª")
        print(f"ğŸ“ Markdown æ–‡ä»¶: {len(md_files)} ä¸ª")
        print(f"ğŸ–¼ï¸  å›¾ç‰‡æ–‡ä»¶: {len(img_files)} ä¸ª")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {len(files)} ä¸ª")
        print(f"ğŸ’¾ æ€»å¤§å°: {total_size/1024:.1f} KB")
        
        # é¡µé¢ç»Ÿè®¡
        print(f"\nğŸ“‹ é¡µé¢è¯¦æƒ…:")
        for page in self.scraped_pages:
            print(f"  - {page['title']}")
            print(f"    URL: {page['url']}")
            print(f"    å†…å®¹: {page['content_length']} å­—ç¬¦")
            print(f"    å›¾ç‰‡: {page['images_downloaded']} ä¸ª")
        
        # ç”ŸæˆJSONæŠ¥å‘Š
        report = {
            'scraping_info': {
                'tool': 'Comprehensive Scraper',
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_pages': len(self.scraped_pages),
                'total_images': len(img_files),
                'total_files': len(files),
                'total_size_bytes': total_size
            },
            'pages': self.scraped_pages,
            'file_counts': {
                'html': len(html_files),
                'markdown': len(md_files),
                'images': len(img_files),
                'total': len(files)
            },
            'directory_structure': {
                'html': str(self.html_dir),
                'markdown': str(self.markdown_dir),
                'assets': str(self.assets_dir),
                'screenshots': str(self.screenshots_dir)
            }
        }
        
        report_path = self.output_dir / 'comprehensive_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        print(f"ğŸ‰ æ‰€æœ‰å†…å®¹å·²ä¿å­˜åˆ°: {self.output_dir}")

def main():
    """ä¸»å‡½æ•°"""
    scraper = ComprehensiveScraper()
    scraper.scrape_website()

if __name__ == "__main__":
    main()
