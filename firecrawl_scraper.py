#!/usr/bin/env python3
"""
Firecrawl MCP å®¢æˆ·ç«¯ - 68tt.co ç½‘ç«™æŠ“å–
ä½¿ç”¨ Firecrawl MCP æœåŠ¡è¿›è¡Œä¸“ä¸šç½‘ç«™å†…å®¹æå–
"""

import asyncio
import json
import os
import subprocess
import sys
from pathlib import Path
import time
import requests
from urllib.parse import urljoin, urlparse
import shutil

class FirecrawlMCPClient:
    def __init__(self, config_file="firecrawl_simple_config.json"):
        self.config_file = config_file
        self.config = self.load_config()
        self.output_dir = Path(self.config['scraping_config']['output']['directory'])
        self.output_dir.mkdir(exist_ok=True)
        self.api_key = None
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            sys.exit(1)
    
    def setup_api_key(self):
        """è®¾ç½®Firecrawl APIå¯†é’¥"""
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        api_key = os.getenv('FIRECRAWL_API_KEY')
        
        if not api_key:
            print("ğŸ”‘ Firecrawl API å¯†é’¥è®¾ç½®")
            print("=" * 40)
            print("è¯·è®¿é—® https://firecrawl.dev è·å–å…è´¹APIå¯†é’¥")
            print("æˆ–è€…ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼ï¼ˆåŠŸèƒ½å—é™ï¼‰")
            print()
            
            choice = input("1) è¾“å…¥APIå¯†é’¥\n2) ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼\nè¯·é€‰æ‹© (1-2): ").strip()
            
            if choice == "1":
                api_key = input("è¯·è¾“å…¥æ‚¨çš„Firecrawl APIå¯†é’¥: ").strip()
                if not api_key:
                    print("âŒ æœªè¾“å…¥APIå¯†é’¥")
                    return False
                    
                # ä¿å­˜åˆ°ç¯å¢ƒå˜é‡
                os.environ['FIRECRAWL_API_KEY'] = api_key
                
                # è¯¢é—®æ˜¯å¦ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
                save_choice = input("æ˜¯å¦ä¿å­˜åˆ°é…ç½®æ–‡ä»¶? (y/n): ").strip().lower()
                if save_choice == 'y':
                    self.config['mcpServers']['firecrawl']['env']['FIRECRAWL_API_KEY'] = api_key
                    with open(self.config_file, 'w', encoding='utf-8') as f:
                        json.dump(self.config, f, indent=2, ensure_ascii=False)
                    print("âœ… APIå¯†é’¥å·²ä¿å­˜åˆ°é…ç½®æ–‡ä»¶")
                    
            elif choice == "2":
                print("âš ï¸  ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼ï¼ŒåŠŸèƒ½å—é™")
                api_key = "demo-key"
                os.environ['FIRECRAWL_API_KEY'] = api_key
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return False
        
        self.api_key = api_key
        return True
    
    def install_firecrawl_mcp(self):
        """å®‰è£…Firecrawl MCPæœåŠ¡å™¨"""
        print("ğŸ“¦ æ£€æŸ¥Firecrawl MCPæœåŠ¡å™¨...")
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
            result = subprocess.run(['npx', '--version'], capture_output=True, text=True)
            if result.returncode != 0:
                print("âŒ NPX æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… Node.js")
                return False
            
            print("âœ… NPX å¯ç”¨")
            
            # æµ‹è¯•Firecrawl MCPæœåŠ¡å™¨
            print("ğŸ”§ å‡†å¤‡Firecrawl MCPæœåŠ¡å™¨...")
            return True
            
        except FileNotFoundError:
            print("âŒ Node.js/NPM æœªå®‰è£…")
            print("è¯·è®¿é—® https://nodejs.org å®‰è£… Node.js")
            return False
    
    def scrape_with_firecrawl_api(self):
        """ä½¿ç”¨Firecrawl APIç›´æ¥æŠ“å–"""
        if not self.api_key or self.api_key == "demo-key":
            print("âš ï¸  ä½¿ç”¨å…è´¹æŠ“å–æ¨¡å¼ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰")
            return self.scrape_without_api()
        
        print("ğŸ”¥ ä½¿ç”¨Firecrawl APIæŠ“å–ç½‘ç«™...")
        
        base_url = "https://api.firecrawl.dev/v0"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        target_url = self.config['scraping_config']['target_url']
        crawl_options = self.config['scraping_config']['crawl_options']
        
        # å¯åŠ¨çˆ¬å–ä»»åŠ¡
        crawl_data = {
            "url": target_url,
            "crawlerOptions": {
                "generateImgAltText": crawl_options.get("generateImgAltText", True),
                "returnOnlyUrls": False,
                "maxDepth": crawl_options.get("maxDepth", 2),
                "limit": crawl_options.get("limit", 5)
            },
            "pageOptions": {
                "onlyMainContent": True,
                "includeHtml": True,
                "screenshot": False
            }
        }
        
        try:
            print(f"ğŸš€ å¼€å§‹çˆ¬å–: {target_url}")
            response = requests.post(f"{base_url}/crawl", json=crawl_data, headers=headers)
            
            if response.status_code == 200:
                job_data = response.json()
                job_id = job_data.get("jobId")
                
                if job_id:
                    print(f"ğŸ“‹ çˆ¬å–ä»»åŠ¡ID: {job_id}")
                    return self.wait_for_crawl_completion(job_id, headers, base_url)
                else:
                    print("âŒ æœªè·å–åˆ°ä»»åŠ¡ID")
                    return False
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
                return False
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            return False
    
    def wait_for_crawl_completion(self, job_id, headers, base_url):
        """ç­‰å¾…çˆ¬å–ä»»åŠ¡å®Œæˆ"""
        print("â³ ç­‰å¾…çˆ¬å–ä»»åŠ¡å®Œæˆ...")
        
        max_attempts = 30
        attempt = 0
        
        while attempt < max_attempts:
            try:
                response = requests.get(f"{base_url}/crawl/status/{job_id}", headers=headers)
                
                if response.status_code == 200:
                    status_data = response.json()
                    status = status_data.get("status")
                    
                    if status == "completed":
                        print("âœ… çˆ¬å–ä»»åŠ¡å®Œæˆ!")
                        return self.process_crawl_results(status_data)
                    elif status == "failed":
                        print("âŒ çˆ¬å–ä»»åŠ¡å¤±è´¥")
                        print(f"é”™è¯¯ä¿¡æ¯: {status_data.get('error', 'Unknown error')}")
                        return False
                    else:
                        completed = status_data.get("completed", 0)
                        total = status_data.get("total", 0)
                        print(f"ğŸ”„ è¿›åº¦: {completed}/{total} ({status})")
                        
                else:
                    print(f"âš ï¸  çŠ¶æ€æ£€æŸ¥å¤±è´¥: {response.status_code}")
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸  çŠ¶æ€æ£€æŸ¥é”™è¯¯: {e}")
            
            attempt += 1
            time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
        
        print("â° ç­‰å¾…è¶…æ—¶")
        return False
    
    def process_crawl_results(self, results_data):
        """å¤„ç†çˆ¬å–ç»“æœ"""
        print("ğŸ“Š å¤„ç†çˆ¬å–ç»“æœ...")
        
        data = results_data.get("data", [])
        if not data:
            print("âŒ æ²¡æœ‰è·å–åˆ°æ•°æ®")
            return False
        
        print(f"ğŸ“„ è·å–åˆ° {len(data)} ä¸ªé¡µé¢")
        
        # ä¿å­˜åŸå§‹ç»“æœ
        raw_results_path = self.output_dir / "firecrawl_raw_results.json"
        with open(raw_results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        # å¤„ç†æ¯ä¸ªé¡µé¢
        processed_pages = []
        
        for i, page_data in enumerate(data, 1):
            url = page_data.get("metadata", {}).get("sourceURL", f"page_{i}")
            title = page_data.get("metadata", {}).get("title", "Untitled")
            
            print(f"ğŸ“ å¤„ç†é¡µé¢ {i}: {title}")
            
            # ä¿å­˜HTML
            if page_data.get("html"):
                html_filename = self.url_to_filename(url) + ".html"
                html_path = self.output_dir / html_filename
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(page_data["html"])
            
            # ä¿å­˜Markdown
            if page_data.get("markdown"):
                md_filename = self.url_to_filename(url) + ".md"
                md_path = self.output_dir / md_filename
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(page_data["markdown"])
            
            # ä¿å­˜ç»“æ„åŒ–æ•°æ®
            page_info = {
                "url": url,
                "title": title,
                "metadata": page_data.get("metadata", {}),
                "content_length": len(page_data.get("markdown", "")),
                "html_length": len(page_data.get("html", "")),
                "extracted_at": time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            processed_pages.append(page_info)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_firecrawl_report(processed_pages, results_data)
        
        print(f"âœ… æ‰€æœ‰é¡µé¢å¤„ç†å®Œæˆï¼Œä¿å­˜åœ¨: {self.output_dir}")
        return True
    
    def scrape_without_api(self):
        """æ— APIå¯†é’¥çš„å¤‡ç”¨æŠ“å–æ–¹æ³•"""
        print("ğŸ”§ ä½¿ç”¨å¤‡ç”¨æŠ“å–æ–¹æ³•...")
        
        # ä½¿ç”¨requestsç›´æ¥æŠ“å–ä¸»è¦é¡µé¢
        target_url = self.config['scraping_config']['target_url']
        pages_to_scrape = [
            target_url,
            urljoin(target_url, '/about'),
            urljoin(target_url, '/privacy')
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        processed_pages = []
        
        for url in pages_to_scrape:
            try:
                print(f"ğŸ“¥ æŠ“å–: {url}")
                response = requests.get(url, headers=headers, timeout=30)
                
                if response.status_code == 200:
                    # ä¿å­˜HTML
                    html_filename = self.url_to_filename(url) + ".html"
                    html_path = self.output_dir / html_filename
                    with open(html_path, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    # æå–åŸºæœ¬ä¿¡æ¯
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.text, 'html.parser')
                    title = soup.title.string if soup.title else "Untitled"
                    
                    page_info = {
                        "url": url,
                        "title": title.strip(),
                        "status_code": response.status_code,
                        "content_length": len(response.text),
                        "extracted_at": time.strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    processed_pages.append(page_info)
                    print(f"âœ… æˆåŠŸ: {title.strip()}")
                    
                else:
                    print(f"âŒ å¤±è´¥ {url}: HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ é”™è¯¯ {url}: {e}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_simple_report(processed_pages)
        return len(processed_pages) > 0
    
    def url_to_filename(self, url):
        """URLè½¬æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        if not path:
            return 'index'
        
        # æ¸…ç†æ–‡ä»¶å
        filename = path.replace('/', '_').replace('?', '_').replace('&', '_')
        return filename or 'index'
    
    def generate_firecrawl_report(self, processed_pages, raw_data):
        """ç”ŸæˆFirecrawlæŠ¥å‘Š"""
        report = {
            "scraping_info": {
                "tool": "Firecrawl MCP",
                "target_url": self.config['scraping_config']['target_url'],
                "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                "total_pages": len(processed_pages),
                "api_key_used": bool(self.api_key and self.api_key != "demo-key")
            },
            "pages": processed_pages,
            "raw_data_summary": {
                "total_data_points": len(raw_data.get("data", [])),
                "job_status": raw_data.get("status", "unknown"),
                "completed": raw_data.get("completed", 0),
                "total": raw_data.get("total", 0)
            }
        }
        
        report_path = self.output_dir / "firecrawl_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def generate_simple_report(self, processed_pages):
        """ç”Ÿæˆç®€å•æŠ¥å‘Š"""
        report = {
            "scraping_info": {
                "tool": "Backup Scraper",
                "target_url": self.config['scraping_config']['target_url'],
                "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                "total_pages": len(processed_pages)
            },
            "pages": processed_pages
        }
        
        report_path = self.output_dir / "scraping_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def run(self):
        """è¿è¡ŒFirecrawl MCPå®¢æˆ·ç«¯"""
        print("ğŸ”¥ Firecrawl MCP 68tt.co ç½‘ç«™æŠ“å–å™¨")
        print("=" * 50)
        
        # æ£€æŸ¥Node.jsç¯å¢ƒ
        if not self.install_firecrawl_mcp():
            return False
        
        # è®¾ç½®APIå¯†é’¥
        if not self.setup_api_key():
            return False
        
        # å¼€å§‹æŠ“å–
        success = self.scrape_with_firecrawl_api()
        
        if success:
            print("\nğŸ‰ Firecrawl æŠ“å–å®Œæˆ!")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
            
            # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶
            if self.output_dir.exists():
                files = list(self.output_dir.glob("*"))
                print(f"\nğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶ ({len(files)} ä¸ª):")
                for file in sorted(files):
                    print(f"   - {file.name}")
        else:
            print("\nâŒ æŠ“å–å¤±è´¥")
        
        return success

def main():
    """ä¸»å‡½æ•°"""
    client = FirecrawlMCPClient()
    client.run()

if __name__ == "__main__":
    main()
